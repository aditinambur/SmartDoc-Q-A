{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditinambur/SmartDoc-Q-A/blob/main/SmartDoc_Q_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK94XxjoUgcw",
        "outputId": "5932c981-1986-412e-9367-9ad06a59884e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "/bin/bash: line 1: 2: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers transformers accelerate PyMuPDF faiss-cpu torch torchvision torchaudio rank_bm25\n",
        "!pip install numpy<2 --upgrade\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MJc3gg7oVqm5"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from google.colab import files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import fitz\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "import os\n",
        "import tempfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d50556",
        "outputId": "d3a6d7c7-30b8-4d8f-b41e-c19025714ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JNd4L0BgV0AJ"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "print(\"Gemini API configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf6TjcSAfRet",
        "outputId": "ad99b4fe-77c7-4af2-8969-dc785008344c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_chunks = []\n",
        "chat_embeddings = []\n",
        "chat_index = faiss.IndexFlatL2(384) #dimension 384\n",
        "chat_bm25_corpus = []\n",
        "chat_bm25_index = None\n",
        "\n",
        "def passability_check(query, threshold=0.7):\n",
        "    if not chat_chunks:\n",
        "        return False\n",
        "\n",
        "    query_vec = model.encode([query]).astype('float32')\n",
        "    D, _ = chat_index.search(query_vec, k=1)\n",
        "    best_sim = 1 - (D[0][0] / 2)  #convert L2 to cosine-like approximation\n",
        "\n",
        "    return best_sim >= threshold\n",
        "\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def embed(text):\n",
        "    x = max(300, min(1000, int(len(text) / 8)))\n",
        "    y = max(50, int(0.15 * x))\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=x, chunk_overlap=y)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    chunk_embeddings = model.encode(chunks)\n",
        "    embedding_matrix = np.array(chunk_embeddings).astype('float32')\n",
        "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "    index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
        "    index.add(embedding_matrix)\n",
        "    tokenized_corpus = [nltk.word_tokenize(doc) for doc in chunks]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    return chunks, index, bm25\n",
        "\n",
        "def gen_prompt(chunks, query):\n",
        "    context = \"\\n\".join(chunks)\n",
        "    prompt = f\"\"\"You are a helpful assistant answering based *only* on the following context (from documents or past conversation).\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def build_prompt(question):\n",
        "    return f\"\"\"Classify the following question into one of these categories specified with numbers:\n",
        "1-‚Äì asks for a specific fact (e.g., \"When was the Constitution signed?\") - 1\n",
        "2-‚Äì can be answered with yes or no (e.g., \"Can a contract be voided under duress?\") - 2\n",
        "6-‚Äì asks for a detailed explanation (e.g., \"How does a merger affect minority shareholders?\") - 6\n",
        "8-- summary ‚Äì asks to summarize a document, event, or content (e.g., \"Summarize the key terms of this agreement.\") - 8\n",
        "\n",
        "\n",
        "Here are a few examples:\n",
        "\n",
        "Q: Who is the current president of India?\n",
        "A: 1\n",
        "\n",
        "Q: Is the law still valid in 2025?\n",
        "A: 2\n",
        "\n",
        "Q: What is inflation?\n",
        "A: 6\n",
        "\n",
        "Q: Summarize the main points of the introduction section.\n",
        "A: 8\n",
        "\n",
        "Now classify the next question and return either 1/2/6/8:\n",
        "\n",
        "Q: {question}\n",
        "A:\"\"\"\n",
        "\n",
        "def classify_question(question):\n",
        "    prompt = build_prompt(question)\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        answer = response.text.strip().split()[0]\n",
        "        answer_num = int(''.join(filter(str.isdigit, answer)))\n",
        "\n",
        "        if answer_num in [1, 2, 6, 8]:\n",
        "            return answer_num\n",
        "        else:\n",
        "            return 4\n",
        "    except Exception as e:\n",
        "        print(f\"Classification error: {e}. Using default value.\")\n",
        "        return 4\n",
        "\n",
        "def rag_answer(query, chunks, model, index, bm25_index, method, gen_model):\n",
        "    global chat_bm25_index\n",
        "\n",
        "    num_chunks = classify_question(query)\n",
        "    max_chunks = len(chunks)\n",
        "    num_chunks = min(num_chunks, max_chunks)\n",
        "\n",
        "    if method == \"RAG (Vector Search)\":\n",
        "        query_encoding = model.encode([query]).astype('float32')\n",
        "        _, indices = index.search(query_encoding, k=num_chunks)\n",
        "        chunk_contexts = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "    elif method == \"BM25\":\n",
        "        tokenized_query = nltk.word_tokenize(query)\n",
        "        scores = bm25_index.get_scores(tokenized_query)\n",
        "        top_n = np.argsort(scores)[::-1][:num_chunks]\n",
        "        chunk_contexts = [chunks[i] for i in top_n if i < len(chunks)]\n",
        "    elif method == \"Both\":\n",
        "        # RAG retrieval\n",
        "        query_encoding = model.encode([query]).astype('float32')\n",
        "        _, rag_indices = index.search(query_encoding, k=num_chunks)\n",
        "        rag_contexts = [chunks[i] for i in rag_indices[0] if i < len(chunks)]\n",
        "\n",
        "        # BM25 retrieval\n",
        "        tokenized_query = nltk.word_tokenize(query)\n",
        "        scores = bm25_index.get_scores(tokenized_query)\n",
        "        bm25_top_n = np.argsort(scores)[::-1][:num_chunks]\n",
        "        bm25_contexts = [chunks[i] for i in bm25_top_n if i < len(chunks)]\n",
        "\n",
        "        # Combine and deduplicate contexts\n",
        "        chunk_contexts = rag_contexts\n",
        "        for context in bm25_contexts:\n",
        "            if context not in chunk_contexts:\n",
        "                chunk_contexts.append(context)\n",
        "    else:\n",
        "        return \"Invalid retrieval method selected.\"\n",
        "\n",
        "    if not chunk_contexts:\n",
        "        return \"No relevant context found for the query.\"\n",
        "\n",
        "    history_chunks = []\n",
        "\n",
        "    if passability_check(query):\n",
        "        if method == \"RAG (Vector Search)\":\n",
        "            query_encoding = model.encode([query]).astype('float32')\n",
        "            _, indices = chat_index.search(query_encoding, k=2)\n",
        "            history_chunks = [chat_chunks[i] for i in indices[0]]\n",
        "        elif method == \"BM25\":\n",
        "            if chat_bm25_index:\n",
        "                tokenized_query = nltk.word_tokenize(query)\n",
        "                scores = chat_bm25_index.get_scores(tokenized_query)\n",
        "                top_n = np.argsort(scores)[::-1][:2]\n",
        "                history_chunks = [chat_chunks[i] for i in top_n]\n",
        "        elif method == \"Both\":\n",
        "            query_encoding = model.encode([query]).astype('float32')\n",
        "            _, rag_indices = chat_index.search(query_encoding, k=2)\n",
        "            rag_chunks = [chat_chunks[i] for i in rag_indices[0]]\n",
        "\n",
        "            if chat_bm25_index:\n",
        "                tokenized_query = nltk.word_tokenize(query)\n",
        "                scores = chat_bm25_index.get_scores(tokenized_query)\n",
        "                bm25_top_n = np.argsort(scores)[::-1][:2]\n",
        "                bm25_chunks = [chat_chunks[i] for i in bm25_top_n]\n",
        "            else:\n",
        "                bm25_chunks = []\n",
        "\n",
        "\n",
        "            history_chunks = rag_chunks\n",
        "            for chunk in bm25_chunks:\n",
        "                if chunk not in history_chunks:\n",
        "                    history_chunks.append(chunk)\n",
        "\n",
        "    full_context = history_chunks + chunk_contexts\n",
        "    prompt = gen_prompt(full_context, query)\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = gen_model.generate_content(prompt)\n",
        "        qa_pair = f\"Q: {query}\\nA: {response.text.strip()}\"\n",
        "        chat_chunks.append(qa_pair)\n",
        "\n",
        "        if len(chat_chunks) > 20:\n",
        "            chat_chunks.pop(0)\n",
        "            chat_embeddings.pop(0)\n",
        "            chat_bm25_corpus.pop(0)\n",
        "\n",
        "        new_embedding = model.encode([qa_pair])[0].astype('float32')\n",
        "        chat_embeddings.append(new_embedding)\n",
        "        chat_index.reset()\n",
        "        chat_index.add(np.array(chat_embeddings))\n",
        "\n",
        "        chat_bm25_corpus.append(nltk.word_tokenize(qa_pair))\n",
        "        chat_bm25_index = BM25Okapi(chat_bm25_corpus)\n",
        "\n",
        "\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def file_upload(file_obj):\n",
        "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
        "        tmp_file.write(file_obj)\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    text = read_pdf(tmp_file_path)\n",
        "\n",
        "    os.remove(tmp_file_path)\n",
        "\n",
        "    chunks, index, bm25_index = embed(text)\n",
        "    return chunks, index, bm25_index\n",
        "\n",
        "def answer_with_rag(query, chunks, index, bm25_index, method):\n",
        "    return rag_answer(query, chunks, model, index, bm25_index, method, gemini_model)\n",
        "\n",
        "print(\"Core functions loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFhTeVGsfScR",
        "outputId": "2e5653af-1434-43d3-ab86-1b1e69d2f461"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core functions loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"RAG Document Query with Gemini\") as demo:\n",
        "    gr.Markdown(\"## üìÑ RAG Document Query with Gemini API\")\n",
        "    gr.Markdown(\"Upload a PDF document and ask questions about its content using different retrieval methods.\")\n",
        "\n",
        "    # State variables to hold chunks and index\n",
        "    chunks_state = gr.State(None)\n",
        "    index_state = gr.State(None)\n",
        "    bm25_index_state = gr.State(None)\n",
        "\n",
        "    # --- File Upload ---\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload your PDF document\", file_types=[\".pdf\"])\n",
        "\n",
        "    process_button = gr.Button(\"üì• Process Document\", variant=\"primary\")\n",
        "    process_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "\n",
        "    # --- Retrieval Method Selection ---\n",
        "    retrieval_method = gr.Radio(\n",
        "        choices=[\"RAG (Vector Search)\", \"BM25\", \"Both\"],\n",
        "        label=\"Select Retrieval Method\",\n",
        "        value=\"RAG (Vector Search)\"\n",
        "    )\n",
        "\n",
        "    # --- Question + Answer ---\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            query_input = gr.Textbox(\n",
        "                lines=3,\n",
        "                placeholder=\"Ask your question here...\",\n",
        "                label=\"üìù Question\"\n",
        "            )\n",
        "            answer_button = gr.Button(\"üîç Get Answer\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(\n",
        "                label=\"üì§ Answer\",\n",
        "                lines=8,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    # --- Actions ---\n",
        "    def process_document(file_obj):\n",
        "        if file_obj is None:\n",
        "            return \"Please upload a PDF file first.\", None, None, None\n",
        "\n",
        "        try:\n",
        "            # Get the file path from the Gradio file object\n",
        "            file_path = file_obj.name\n",
        "            # Read the file content as bytes from the path\n",
        "            with open(file_path, 'rb') as f:\n",
        "                file_content = f.read()\n",
        "\n",
        "            chunks, index, bm25_index = file_upload(file_content)\n",
        "            return f\"Document processed successfully! Created {len(chunks)} chunks.\", chunks, index, bm25_index\n",
        "        except Exception as e:\n",
        "            return f\"Error processing document: {str(e)}\", None, None, None\n",
        "\n",
        "    def get_answer(query, chunks, index, bm25_index, method):\n",
        "        if chunks is None or index is None or bm25_index is None:\n",
        "            return \"Please process a document first.\"\n",
        "\n",
        "        if not query.strip():\n",
        "            return \"Please enter a question.\"\n",
        "\n",
        "        try:\n",
        "            answer = answer_with_rag(query, chunks, index, bm25_index, method)\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_document,\n",
        "        inputs=[file_input],\n",
        "        outputs=[process_status, chunks_state, index_state, bm25_index_state]\n",
        "    )\n",
        "\n",
        "    answer_button.click(\n",
        "        fn=get_answer,\n",
        "        inputs=[query_input, chunks_state, index_state, bm25_index_state, retrieval_method],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "    # Some examples\n",
        "    gr.Markdown(\"### Example Questions:\")\n",
        "    gr.Markdown(\"- What are the main points discussed in this document?\")\n",
        "    gr.Markdown(\"- Summarize the key findings.\")\n",
        "    gr.Markdown(\"- What is the conclusion of this document?\")\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)\n",
        "\n",
        "print(\"Gradio interface launched!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "GSJ2u1_pfuKW",
        "outputId": "63a8967c-014c-441e-f962-0bb7b93c99bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Textbox.__init__() got an unexpected keyword argument 'allow_copy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1052877072.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             output_text = gr.Textbox(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"üì§ Answer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Textbox.__init__() got an unexpected keyword argument 'allow_copy'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}